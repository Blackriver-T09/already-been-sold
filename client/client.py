import cv2
import numpy as np
import base64
import time
import threading
import socketio
import json
from queue import Queue, Empty
import pygame  # ğŸ†• æ·»åŠ éŸ³é¢‘æ’­æ”¾åº“
import os

class FaceEmotionClient:
    """äººè„¸æƒ…æ„Ÿè¯†åˆ«WebSocketå®¢æˆ·ç«¯"""
    
    def __init__(self, server_url='http://frp-hub.com:45170'):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯
        
        å‚æ•°:
            server_url: æœåŠ¡å™¨åœ°å€
        """
        self.server_url = server_url
        
        # Socket.IOå®¢æˆ·ç«¯é…ç½®
        self.sio = socketio.Client(
            logger=False,
            engineio_logger=False,
            reconnection=True,
            reconnection_attempts=5,
            reconnection_delay=1,
            reconnection_delay_max=5
        )
        
        self.camera = None
        self.is_running = False
        self.is_connected = False
        
        # æ˜¾ç¤ºç›¸å…³
        self.display_frame = None
        self.frame_lock = threading.Lock()
        
        # ğŸ†• ç‰¹æ®Šæ˜¾ç¤ºæ¨¡å¼
        self.special_display_mode = False
        self.special_display_image = None
        self.special_display_end_time = 0
        
        # ğŸ†• éŸ³ç”»åŒæ­¥ç›¸å…³
        self.audio_display_sync_ready = False
        self.pending_composed_image = None
        self.pending_display_duration = 0
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'frames_sent': 0,
            'frames_received': 0,
            'last_send_time': 0,
            'last_receive_time': 0,
            'avg_latency': 0,
            'connection_time': 0
        }
        
        # è°ƒè¯•æ§åˆ¶
        self.debug_mode = False
        self.debug_frame_count = 0
        
        # ğŸ†• åˆå§‹åŒ–éŸ³é¢‘ç³»ç»Ÿ
        self._init_audio()
        
        # è®¾ç½®WebSocketäº‹ä»¶å¤„ç†
        self._setup_socket_events()
        
        print("ğŸ–¥ï¸ å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
    
    def _init_audio(self):
        """åˆå§‹åŒ–éŸ³é¢‘ç³»ç»Ÿ"""
        try:
            pygame.mixer.init()
            
            # ğŸ†• åŠ è½½æ‹ç…§éŸ³æ•ˆ
            audio_path = os.path.join("sources", "voice", "camera.mp3")
            if os.path.exists(audio_path):
                self.camera_sound = pygame.mixer.Sound(audio_path)
                print(f"âœ… æ‹ç…§éŸ³æ•ˆåŠ è½½æˆåŠŸ: {audio_path}")
            else:
                print(f"âš ï¸ æ‹ç…§éŸ³æ•ˆæ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
                self.camera_sound = None
        except Exception as e:
            print(f"âŒ éŸ³é¢‘ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            self.camera_sound = None
    
    def play_camera_sound(self):
        """æ’­æ”¾æ‹ç…§éŸ³æ•ˆ"""
        try:
            if self.camera_sound:
                self.camera_sound.play()
                print("ğŸ”Š æ’­æ”¾æ‹ç…§éŸ³æ•ˆ")
        except Exception as e:
            print(f"âŒ æ’­æ”¾éŸ³æ•ˆå¤±è´¥: {e}")
    
    def _setup_socket_events(self):
        """è®¾ç½®WebSocketäº‹ä»¶å¤„ç†"""
        
        @self.sio.event
        def connect():
            print("ğŸ”— å·²è¿æ¥åˆ°æœåŠ¡å™¨")
            self.is_connected = True
            self.stats['connection_time'] = time.time()
        
        @self.sio.event
        def disconnect():
            print("âŒ ä¸æœåŠ¡å™¨æ–­å¼€è¿æ¥")
            self.is_connected = False
            if self.is_running:
                print("ğŸ”„ å°è¯•é‡æ–°è¿æ¥...")
                threading.Timer(2.0, self._reconnect).start()
        
        @self.sio.event
        def connect_error(data):
            print(f"âŒ è¿æ¥é”™è¯¯: {data}")
        
        @self.sio.event
        def connection_response(data):
            print(f"âœ… æœåŠ¡å™¨å“åº”: {data}")
        
        @self.sio.event
        def processed_frame(data):
            """æ¥æ”¶å¤„ç†åçš„å¸§"""
            try:
                if data.get('success'):
                    # å¤§å¹…å‡å°‘è°ƒè¯•è¾“å‡ºé¢‘ç‡
                    self.debug_frame_count += 1
                    should_debug = self.debug_mode and (self.debug_frame_count % 60 == 0)
                    
                    if should_debug:
                        print(f"ğŸ”§ DEBUG: å¤„ç†ç¬¬{self.debug_frame_count}å¸§")
                    
                    # è§£ç å¤„ç†åçš„å›¾åƒ
                    processed_image_b64 = data.get('processed_image')
                    if not processed_image_b64:
                        if should_debug:
                            print("âŒ DEBUG: æ²¡æœ‰processed_imageå­—æ®µ")
                        return
                    
                    try:
                        image_bytes = base64.b64decode(processed_image_b64)
                        nparr = np.frombuffer(image_bytes, np.uint8)
                        processed_frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
                        
                        if should_debug:
                            print(f"ğŸ”§ DEBUG: è§£ç å›¾åƒå°ºå¯¸: {processed_frame.shape if processed_frame is not None else 'None'}")
                        
                    except Exception as decode_error:
                        print(f"âŒ å›¾åƒè§£ç å¤±è´¥: {decode_error}")
                        return
                    
                    if processed_frame is not None:
                        # ğŸ”§ å¿«é€Ÿæ›´æ–°æ˜¾ç¤ºå¸§ï¼Œé¿å…é”ç«äº‰
                        with self.frame_lock:
                            # ğŸ†• æ£€æŸ¥æ˜¯å¦åœ¨ç‰¹æ®Šæ˜¾ç¤ºæ¨¡å¼
                            if not self.special_display_mode:
                                self.display_frame = processed_frame
                        
                        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                        self.stats['frames_received'] += 1
                        self.stats['last_receive_time'] = time.time()
                        
                        # è®¡ç®—å»¶è¿Ÿ
                        if 'timestamp' in data:
                            latency = time.time() - data['timestamp']
                            self._update_latency(latency)
                        
                        # å‡å°‘æ‰“å°é¢‘ç‡
                        if self.stats['frames_received'] % 60 == 0:
                            faces_count = len(data.get('faces', []))
                            processing_time = data.get('processing_time', 0)
                            print(f"ğŸ“¸ æ¥æ”¶å¸§: {self.stats['frames_received']}, äººè„¸æ•°={faces_count}, å¤„ç†æ—¶é—´={processing_time:.3f}s")
                    
                else:
                    print(f"âŒ æœåŠ¡å™¨å¤„ç†é”™è¯¯: {data.get('error', 'æœªçŸ¥é”™è¯¯')}")
                    
            except Exception as e:
                print(f"âŒ å¤„ç†æœåŠ¡å™¨å“åº”æ—¶å‡ºé”™: {str(e)}")
        
        # ğŸ†• æ·»åŠ æ‹ç…§äº‹ä»¶å¤„ç†
        @self.sio.event
        def photo_taken(data):
            """å¤„ç†æ‹ç…§äº‹ä»¶"""
            print("ğŸ“¸ æœåŠ¡å™¨é€šçŸ¥ï¼šç…§ç‰‡æ‹æ‘„å®Œæˆï¼")
            self.play_camera_sound()  # æ’­æ”¾æ‹ç…§éŸ³æ•ˆ
        
        # ğŸ†• æ·»åŠ éŸ³ç”»åŒæ­¥äº‹ä»¶å¤„ç†å™¨
        @self.sio.event
        def audio_and_display_sync(data):
            """å¤„ç†éŸ³é¢‘å’Œå›¾ç‰‡å±•ç¤ºåŒæ­¥äº‹ä»¶"""
            try:
                comment = data.get('comment', '')
                audio_filename = data.get('audio_filename', '')
                audio_data = data.get('audio_data', '')  # base64ç¼–ç çš„éŸ³é¢‘æ•°æ®
                photo_path = data.get('photo_path', '')
                emotion_type = data.get('emotion_type', '')
                start_display = data.get('start_display', False)
                
                print(f"ğŸµ æ”¶åˆ°éŸ³ç”»åŒæ­¥ä¿¡å·: {comment}")
                print(f"ğŸ”Š éŸ³é¢‘æ–‡ä»¶: {audio_filename}")
                
                if audio_data and start_display:
                    # è§£ç å¹¶ä¿å­˜éŸ³é¢‘æ–‡ä»¶
                    try:
                        import base64
                        import os
                        
                        # åˆ›å»ºæœ¬åœ°éŸ³é¢‘ç›®å½•
                        local_audio_dir = 'received_audio'
                        if not os.path.exists(local_audio_dir):
                            os.makedirs(local_audio_dir)
                        
                        # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
                        local_audio_path = os.path.join(local_audio_dir, audio_filename)
                        audio_bytes = base64.b64decode(audio_data)
                        with open(local_audio_path, 'wb') as f:
                            f.write(audio_bytes)
                        
                        print(f"ğŸ’¾ éŸ³é¢‘æ–‡ä»¶å·²ä¿å­˜: {local_audio_path}")
                        
                        # ç«‹å³æ’­æ”¾éŸ³é¢‘
                        self.play_generated_audio(local_audio_path)
                        
                        # è®¾ç½®éŸ³é¢‘åŒæ­¥æ ‡å¿—
                        self.audio_display_sync_ready = True
                        
                        # ğŸ†• æ£€æŸ¥æ˜¯å¦æœ‰ç­‰å¾…ä¸­çš„åˆæˆå›¾ç‰‡ï¼Œå¦‚æœæœ‰åˆ™ç«‹å³å±•ç¤º
                        if self.pending_composed_image is not None:
                            print(f"ğŸ¬ éŸ³é¢‘å¼€å§‹æ’­æ”¾ï¼ŒåŒæ­¥å±•ç¤ºç­‰å¾…ä¸­çš„åˆæˆå›¾ç‰‡")
                            self._start_display_composed_image(self.pending_composed_image, self.pending_display_duration)
                        else:
                            print(f"âœ… éŸ³ç”»åŒæ­¥å°±ç»ªï¼Œç­‰å¾…åˆæˆå›¾ç‰‡...")
                        
                    except Exception as audio_error:
                        print(f"âŒ éŸ³é¢‘å¤„ç†å¤±è´¥: {str(audio_error)}")
                        # å³ä½¿éŸ³é¢‘å¤„ç†å¤±è´¥ï¼Œä¹Ÿè¦å…è®¸å›¾ç‰‡å±•ç¤º
                        self.audio_display_sync_ready = True
                
            except Exception as e:
                print(f"âŒ å¤„ç†éŸ³ç”»åŒæ­¥äº‹ä»¶æ—¶å‡ºé”™: {str(e)}")
        
        # ğŸ†• æ·»åŠ åˆæˆå®Œæˆäº‹ä»¶å¤„ç†ï¼ˆéŸ³ç”»åŒæ­¥ç‰ˆæœ¬ï¼‰
        @self.sio.event
        def photo_composed(data):
            """å¤„ç†ç…§ç‰‡åˆæˆå®Œæˆäº‹ä»¶ï¼ˆç­‰å¾…éŸ³é¢‘åŒæ­¥ï¼‰"""
            try:
                print("ğŸ¨ æœåŠ¡å™¨é€šçŸ¥ï¼šç…§ç‰‡åˆæˆå®Œæˆï¼")
                
                composed_image_b64 = data.get('composed_image')
                display_duration = data.get('display_duration', 5)
                
                if composed_image_b64:
                    # è§£ç åˆæˆåçš„å›¾åƒ
                    image_bytes = base64.b64decode(composed_image_b64)
                    nparr = np.frombuffer(image_bytes, np.uint8)
                    composed_frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
                    
                    if composed_frame is not None:
                        # ğŸ†• æ£€æŸ¥æ˜¯å¦å·²æ”¶åˆ°éŸ³é¢‘åŒæ­¥ä¿¡å·
                        if self.audio_display_sync_ready:
                            # éŸ³é¢‘å·²å°±ç»ªï¼Œç«‹å³å±•ç¤ºå›¾ç‰‡
                            print(f"âœ… éŸ³ç”»åŒæ­¥å°±ç»ªï¼Œç«‹å³å±•ç¤ºåˆæˆå›¾ç‰‡ {display_duration} ç§’")
                            self._start_display_composed_image(composed_frame, display_duration)
                        else:
                            # éŸ³é¢‘è¿˜æœªå°±ç»ªï¼Œæš‚å­˜å›¾ç‰‡ç­‰å¾…åŒæ­¥
                            print(f"ğŸ•°ï¸ ç­‰å¾…éŸ³é¢‘åŒæ­¥ä¿¡å·ï¼Œæš‚å­˜åˆæˆå›¾ç‰‡...")
                            self.pending_composed_image = composed_frame
                            self.pending_display_duration = display_duration
                            
                            # è®¾ç½®è¶…æ—¶æœºåˆ¶ï¼Œå¦‚æœ10ç§’å†…æ²¡æœ‰æ”¶åˆ°éŸ³é¢‘åŒæ­¥ï¼Œå°±ç›´æ¥å±•ç¤º
                            def timeout_display():
                                time.sleep(10)
                                if self.pending_composed_image is not None and not self.audio_display_sync_ready:
                                    print(f"âš ï¸ éŸ³é¢‘åŒæ­¥è¶…æ—¶ï¼Œç›´æ¥å±•ç¤ºåˆæˆå›¾ç‰‡")
                                    self._start_display_composed_image(self.pending_composed_image, self.pending_display_duration)
                                    self.pending_composed_image = None
                                    self.pending_display_duration = 0
                            
                            timeout_thread = threading.Thread(target=timeout_display)
                            timeout_thread.daemon = True
                            timeout_thread.start()
                    
            except Exception as e:
                print(f"âŒ å¤„ç†åˆæˆå›¾ç‰‡æ—¶å‡ºé”™: {str(e)}")
        
        @self.sio.event
        def error(data):
            print(f"âš ï¸ æœåŠ¡å™¨é”™è¯¯: {data}")
        
        @self.sio.event
        def pong(data):
            """å¤„ç†pongå“åº”"""
            server_time = data.get('server_timestamp', 0)
            client_time = data.get('client_timestamp', 0)
            current_time = time.time()
            
            if client_time > 0:
                round_trip_time = current_time - client_time
                print(f"ğŸ“ Pingå»¶è¿Ÿ: {round_trip_time*1000:.1f}ms")
        
        @self.sio.event
        def stats_response(data):
            """å¤„ç†ç»Ÿè®¡ä¿¡æ¯å“åº”"""
            print("ğŸ“Š æœåŠ¡å™¨ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   è¿æ¥å®¢æˆ·ç«¯: {data.get('connected_clients', 0)}")
            print(f"   å¤„ç†ç»Ÿè®¡: {data.get('processing_stats', {})}")
    
    def play_camera_sound(self):
        """æ’­æ”¾æ‹ç…§éŸ³æ•ˆ"""
        try:
            # åˆå§‹åŒ–pygame mixerï¼ˆå¦‚æœè¿˜æ²¡æœ‰åˆå§‹åŒ–ï¼‰
            if not pygame.mixer.get_init():
                pygame.mixer.init()
            
            # æŸ¥æ‰¾æ‹ç…§éŸ³æ•ˆæ–‡ä»¶
            camera_sound_path = os.path.join('sources', 'voice', 'camera.mp3')
            
            if os.path.exists(camera_sound_path):
                print(f"ğŸ”Š æ’­æ”¾æ‹ç…§éŸ³æ•ˆ: {camera_sound_path}")
                
                # ä½¿ç”¨pygameæ’­æ”¾éŸ³æ•ˆï¼ˆéé˜»å¡ï¼‰
                pygame.mixer.music.load(camera_sound_path)
                pygame.mixer.music.play()
                
                # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿æ’­æ”¾å¼€å§‹ï¼Œä½†ä¸é˜»å¡
                time.sleep(0.1)
                
            else:
                print(f"âš ï¸ æ‹ç…§éŸ³æ•ˆæ–‡ä»¶ä¸å­˜åœ¨: {camera_sound_path}")
                print("ğŸ”Š æ’­æ”¾é»˜è®¤æ‹ç…§éŸ³æ•ˆ")
            
        except Exception as e:
            print(f"âŒ æ’­æ”¾æ‹ç…§éŸ³æ•ˆå¤±è´¥: {str(e)}")
    
    def play_generated_audio(self, audio_path):
        """æ’­æ”¾ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶ï¼ˆä½¿ç”¨Soundå¯¹è±¡é¿å…ä¸æ‹ç…§éŸ³æ•ˆå†²çªï¼‰"""
        try:
            # åˆå§‹åŒ–pygame mixerï¼ˆå¦‚æœè¿˜æ²¡æœ‰åˆå§‹åŒ–ï¼‰
            if not pygame.mixer.get_init():
                pygame.mixer.init()
            
            if os.path.exists(audio_path):
                print(f"ğŸµ å¼€å§‹æ’­æ”¾è¯„ä»·éŸ³é¢‘: {audio_path}")
                
                # ä½¿ç”¨Soundå¯¹è±¡æ’­æ”¾ï¼Œé¿å…ä¸musicé€šé“å†²çª
                sound = pygame.mixer.Sound(audio_path)
                sound_channel = sound.play()
                
                # ç­‰å¾…æ’­æ”¾å®Œæˆï¼ˆéé˜»å¡ï¼‰
                def wait_for_audio():
                    while sound_channel and sound_channel.get_busy():
                        time.sleep(0.1)
                    print(f"âœ… è¯„ä»·éŸ³é¢‘æ’­æ”¾å®Œæˆ: {audio_path}")
                
                # åœ¨æ–°çº¿ç¨‹ä¸­ç­‰å¾…æ’­æ”¾å®Œæˆ
                audio_thread = threading.Thread(target=wait_for_audio)
                audio_thread.daemon = True
                audio_thread.start()
                
            else:
                print(f"âš ï¸ è¯„ä»·éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
                
        except Exception as e:
            print(f"âŒ æ’­æ”¾è¯„ä»·éŸ³é¢‘å¤±è´¥: {str(e)}")
    
    def _start_display_composed_image(self, composed_frame, display_duration):
        """å¼€å§‹å±•ç¤ºåˆæˆå›¾ç‰‡"""
        try:
            print(f"ğŸ–¼ï¸ å¼€å§‹å±•ç¤ºåˆæˆå›¾ç‰‡ {display_duration} ç§’")
            
            with self.frame_lock:
                self.special_display_mode = True
                self.special_display_image = composed_frame
                self.special_display_end_time = time.time() + display_duration
                # æ¸…ç†æš‚å­˜çš„å›¾ç‰‡
                self.pending_composed_image = None
                self.pending_display_duration = 0
            
            # è®¾ç½®å®šæ—¶å™¨è‡ªåŠ¨é€€å‡ºç‰¹æ®Šæ˜¾ç¤ºæ¨¡å¼
            threading.Timer(display_duration, self._exit_special_display).start()
            
        except Exception as e:
            print(f"âŒ å±•ç¤ºåˆæˆå›¾ç‰‡å¤±è´¥: {str(e)}")
    
    def _exit_special_display(self):
        """é€€å‡ºç‰¹æ®Šæ˜¾ç¤ºæ¨¡å¼"""
        with self.frame_lock:
            self.special_display_mode = False
            self.special_display_image = None
            # é‡ç½®éŸ³ç”»åŒæ­¥æ ‡å¿—
            self.audio_display_sync_ready = False
            self.pending_composed_image = None
            self.pending_display_duration = 0
        print("ğŸ”„ è¿”å›å®æ—¶è¯†åˆ«ç•Œé¢")
    
    def _reconnect(self):
        """è‡ªåŠ¨é‡è¿"""
        if self.is_running and not self.is_connected:
            try:
                print("ğŸ”„ å°è¯•é‡æ–°è¿æ¥åˆ°æœåŠ¡å™¨...")
                self.sio.connect(self.server_url, 
                               transports=['websocket', 'polling'],
                               wait_timeout=10)
            except Exception as e:
                print(f"âŒ é‡è¿å¤±è´¥: {str(e)}")
    
    def _update_latency(self, latency):
        """æ›´æ–°å»¶è¿Ÿç»Ÿè®¡"""
        alpha = 0.1
        if self.stats['avg_latency'] == 0:
            self.stats['avg_latency'] = latency
        else:
            self.stats['avg_latency'] = (
                alpha * latency + (1 - alpha) * self.stats['avg_latency']
            )
    
    def connect_to_server(self):
        """è¿æ¥åˆ°æœåŠ¡å™¨"""
        try:
            print(f"ğŸ”„ æ­£åœ¨è¿æ¥åˆ°æœåŠ¡å™¨: {self.server_url}")
            
            self.sio.connect(
                self.server_url,
                transports=['websocket', 'polling'],
                wait_timeout=10
            )
            
            time.sleep(1)
            
            if self.sio.connected:
                print("âœ… è¿æ¥æˆåŠŸ")
                return True
            else:
                print("âŒ è¿æ¥å¤±è´¥")
                return False
                
        except Exception as e:
            print(f"âŒ è¿æ¥æœåŠ¡å™¨å¤±è´¥: {str(e)}")
            return False
    
    def disconnect_from_server(self):
        """æ–­å¼€æœåŠ¡å™¨è¿æ¥"""
        try:
            if self.sio.connected:
                self.sio.disconnect()
                print("âœ… å·²æ–­å¼€æœåŠ¡å™¨è¿æ¥")
        except Exception as e:
            print(f"âš ï¸ æ–­å¼€è¿æ¥æ—¶å‡ºé”™: {str(e)}")
    
    def init_camera(self, camera_id=0):
        """åˆå§‹åŒ–æ‘„åƒå¤´"""
        try:
            self.camera = cv2.VideoCapture(camera_id)
            
            if not self.camera.isOpened():
                print(f"âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {camera_id}")
                return False
            
            # è®¾ç½®æ‘„åƒå¤´å‚æ•°
            self.camera.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
            self.camera.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
            self.camera.set(cv2.CAP_PROP_FPS, 30)
            
            print(f"âœ… æ‘„åƒå¤´åˆå§‹åŒ–æˆåŠŸ: {camera_id}")
            return True
            
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–æ‘„åƒå¤´å¤±è´¥: {str(e)}")
            return False
    
    def capture_and_send_frames(self):
        """æ•è·å¹¶å‘é€è§†é¢‘å¸§"""
        frame_id = 0
        target_fps = 10
        frame_interval = 1.0 / target_fps
        
        print(f"ğŸ“¹ å¼€å§‹æ•è·è§†é¢‘å¸§ (ç›®æ ‡FPS: {target_fps})")
        
        while self.is_running and self.camera is not None:
            try:
                current_time = time.time()
                if current_time - self.stats['last_send_time'] < frame_interval:
                    time.sleep(0.01)
                    continue
                
                ret, frame = self.camera.read()
                if not ret:
                    print("âš ï¸ æ— æ³•è¯»å–æ‘„åƒå¤´å¸§")
                    time.sleep(0.1)
                    continue
                
                frame = cv2.flip(frame, 1)
                
                # ğŸ”§ å§‹ç»ˆæ›´æ–°æœ¬åœ°æ˜¾ç¤ºå¸§ï¼ˆä½œä¸ºå¤‡ç”¨ï¼‰
                with self.frame_lock:
                    if self.display_frame is None:
                        self.display_frame = frame.copy()
                
                if not self.is_connected:
                    time.sleep(0.033)
                    continue
                
                _, buffer = cv2.imencode('.jpg', frame, [cv2.IMWRITE_JPEG_QUALITY, 60])
                image_b64 = base64.b64encode(buffer).decode('utf-8')
                
                frame_data = {
                    'frame_id': frame_id,
                    'timestamp': current_time,
                    'image': image_b64,
                    'client_stats': {
                        'frames_sent': self.stats['frames_sent'],
                        'avg_latency': self.stats['avg_latency']
                    }
                }
                
                try:
                    self.sio.emit('video_frame', frame_data)
                    
                    self.stats['frames_sent'] += 1
                    self.stats['last_send_time'] = current_time
                    frame_id += 1
                except Exception as send_error:
                    if self.debug_mode:
                        print(f"âŒ å‘é€å¸§å¤±è´¥: {send_error}")
                    time.sleep(0.1)
                
            except Exception as e:
                print(f"âŒ å‘é€è§†é¢‘å¸§æ—¶å‡ºé”™: {str(e)}")
                time.sleep(0.1)
    
    def display_frames(self):
        """æ˜¾ç¤ºè§†é¢‘å¸§"""
        print("ğŸ–¥ï¸ å¼€å§‹æ˜¾ç¤ºçº¿ç¨‹")
        
        # åœ¨æ˜¾ç¤ºçº¿ç¨‹ä¸­åˆ›å»ºçª—å£
        try:
            cv2.namedWindow("Face Emotion Recognition Client", cv2.WINDOW_NORMAL)
            cv2.resizeWindow("Face Emotion Recognition Client", 800, 600)
            print("âœ… æ˜¾ç¤ºçª—å£åˆ›å»ºæˆåŠŸ")
        except Exception as window_error:
            print(f"âŒ åˆ›å»ºæ˜¾ç¤ºçª—å£å¤±è´¥: {window_error}")
            return
        
        display_count = 0
        last_status_print = 0
        
        while self.is_running:
            try:
                display_count += 1
                current_time = time.time()
                
                # ğŸ†• è·å–è¦æ˜¾ç¤ºçš„å¸§ï¼ˆæ”¯æŒç‰¹æ®Šæ˜¾ç¤ºæ¨¡å¼ï¼‰
                display_copy = None
                with self.frame_lock:
                    if self.special_display_mode and self.special_display_image is not None:
                        # ğŸŒŸ ç‰¹æ®Šæ˜¾ç¤ºæ¨¡å¼ï¼šæ˜¾ç¤ºåˆæˆå›¾ç‰‡
                        display_copy = self.special_display_image.copy()
                        
                        # ğŸ†• æ·»åŠ å€’è®¡æ—¶æ˜¾ç¤º
                        remaining_time = max(0, self.special_display_end_time - current_time)
                        if remaining_time > 0:
                            countdown_text = f"å±•ç¤ºä¸­... {remaining_time:.1f}s"
                            cv2.putText(display_copy, countdown_text, (10, display_copy.shape[0] - 50), 
                                      cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 3)
                        
                    elif self.display_frame is not None:
                        # ğŸ”„ æ­£å¸¸æ¨¡å¼ï¼šæ˜¾ç¤ºå®æ—¶è¯†åˆ«ç”»é¢
                        display_copy = self.display_frame.copy()
                
                if display_copy is None:
                    # åˆ›å»ºç­‰å¾…ç”»é¢
                    display_copy = np.zeros((480, 640, 3), dtype=np.uint8)
                    cv2.putText(display_copy, "Waiting for server response...", 
                              (80, 240), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)
                
                # ğŸ†• åªåœ¨æ­£å¸¸æ¨¡å¼ä¸‹æ·»åŠ çŠ¶æ€ä¿¡æ¯
                if not self.special_display_mode:
                    self._draw_status_info(display_copy)
                
                # æ˜¾ç¤ºå›¾åƒ
                cv2.imshow("Face Emotion Recognition Client", display_copy)
                
                # å¿…é¡»è°ƒç”¨waitKeyï¼Œå¦åˆ™çª—å£æ— æ³•å“åº”
                key = cv2.waitKey(1) & 0xFF
                
                # å¤„ç†æŒ‰é”®
                if key == 27 or key == ord('q'):  # ESCæˆ–Qé€€å‡º
                    print("ğŸ”„ ç”¨æˆ·è¯·æ±‚é€€å‡º")
                    self.stop()
                    break
                elif key == ord('p'):  # På‘é€ping
                    if self.is_connected:
                        self.sio.emit('ping', {'timestamp': time.time()})
                        print("ğŸ“ å‘é€Ping")
                elif key == ord('s'):  # Sè·å–ç»Ÿè®¡ä¿¡æ¯
                    if self.is_connected:
                        self.sio.emit('get_stats')
                        print("ğŸ“Š è·å–ç»Ÿè®¡ä¿¡æ¯")
                elif key == ord('d'):  # Dé”®åˆ‡æ¢è°ƒè¯•æ¨¡å¼
                    self.debug_mode = not self.debug_mode
                    print(f"ğŸ”§ è°ƒè¯•æ¨¡å¼: {'å¼€å¯' if self.debug_mode else 'å…³é—­'}")
                elif key == ord('t'):  # ğŸ†• Té”®æµ‹è¯•éŸ³æ•ˆ
                    self.play_camera_sound()
                    print("ğŸ”Š æµ‹è¯•éŸ³æ•ˆæ’­æ”¾")
                
                # å®šæœŸæ‰“å°çŠ¶æ€ï¼ˆä¸è¦å¤ªé¢‘ç¹ï¼‰
                if current_time - last_status_print > 5.0:  # æ¯5ç§’æ‰“å°ä¸€æ¬¡
                    mode_info = "ç‰¹æ®Šæ˜¾ç¤º" if self.special_display_mode else "å®æ—¶è¯†åˆ«"
                    print(f"ğŸ“º æ˜¾ç¤ºçŠ¶æ€: å¸§{display_count}, æ¨¡å¼={mode_info}, è¿æ¥={self.is_connected}, å‘é€={self.stats['frames_sent']}, æ¥æ”¶={self.stats['frames_received']}")
                    last_status_print = current_time
                
                # æ§åˆ¶æ˜¾ç¤ºé¢‘ç‡ï¼Œé¿å…CPUå ç”¨è¿‡é«˜
                time.sleep(0.030)  # çº¦33fpsæ˜¾ç¤ºé¢‘ç‡
                
            except Exception as e:
                print(f"âŒ æ˜¾ç¤ºè§†é¢‘å¸§æ—¶å‡ºé”™: {str(e)}")
                time.sleep(0.1)
        
        print("ğŸ”„ æ˜¾ç¤ºçº¿ç¨‹ç»“æŸ")
    
    def _draw_status_info(self, image):
        """åœ¨å›¾åƒä¸Šç»˜åˆ¶çŠ¶æ€ä¿¡æ¯"""
        height, width = image.shape[:2]
        
        # è¿æ¥çŠ¶æ€
        status_text = "Connected" if self.is_connected else "Disconnected"
        status_color = (0, 255, 0) if self.is_connected else (0, 0, 255)
        cv2.putText(image, status_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, status_color, 2)
        
        # ç»Ÿè®¡ä¿¡æ¯
        if self.is_connected:
            stats_text = f"Sent: {self.stats['frames_sent']} | Received: {self.stats['frames_received']}"
            cv2.putText(image, stats_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
            
            latency_text = f"Latency: {self.stats['avg_latency']*1000:.1f}ms"
            cv2.putText(image, latency_text, (10, 85), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        # ğŸ†• å¢å¼ºæ§åˆ¶æç¤º
        controls_text = "Keys: ESC/Q(Exit) | P(Ping) | S(Stats) | D(Debug) | T(Test Audio)"
        cv2.putText(image, controls_text, (10, height-20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
    
    def start(self):
        """å¯åŠ¨å®¢æˆ·ç«¯"""
        print("ğŸš€ å¯åŠ¨äººè„¸æƒ…æ„Ÿè¯†åˆ«å®¢æˆ·ç«¯...")
        
        # åˆå§‹åŒ–æ‘„åƒå¤´
        if not self.init_camera():
            print("âŒ æ— æ³•åˆå§‹åŒ–æ‘„åƒå¤´")
            return False
        
        # è¿æ¥æœåŠ¡å™¨
        if not self.connect_to_server():
            print("âŒ æ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ï¼Œå°†åœ¨ç¦»çº¿æ¨¡å¼ä¸‹è¿è¡Œ")
        
        self.is_running = True
        
        # å¯åŠ¨çº¿ç¨‹
        capture_thread = threading.Thread(target=self.capture_and_send_frames)
        display_thread = threading.Thread(target=self.display_frames)
        
        capture_thread.daemon = True
        display_thread.daemon = True
        
        capture_thread.start()
        display_thread.start()
        
        print("âœ… å®¢æˆ·ç«¯å¯åŠ¨å®Œæˆ")
        print("ğŸ“‹ æ§åˆ¶è¯´æ˜:")
        print("   ESCæˆ–Qé”®: é€€å‡ºç¨‹åº")
        print("   Pé”®: å‘é€Pingæµ‹è¯•å»¶è¿Ÿ")
        print("   Sé”®: è·å–æœåŠ¡å™¨ç»Ÿè®¡ä¿¡æ¯")
        print("   Dé”®: åˆ‡æ¢è°ƒè¯•æ¨¡å¼")
        
        try:
            display_thread.join()
        except KeyboardInterrupt:
            print("\nğŸ”„ æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·...")
        finally:
            self.stop()
        
        return True
    
    def stop(self):
        """åœæ­¢å®¢æˆ·ç«¯"""
        print("ğŸ”„ æ­£åœ¨åœæ­¢å®¢æˆ·ç«¯...")
        
        self.is_running = False
        
        # æ–­å¼€æœåŠ¡å™¨è¿æ¥
        self.disconnect_from_server()
        
        # é‡Šæ”¾æ‘„åƒå¤´
        if self.camera is not None:
            self.camera.release()
            self.camera = None
        
        # å…³é—­æ˜¾ç¤ºçª—å£
        cv2.destroyAllWindows()
        
        print("âœ… å®¢æˆ·ç«¯å·²åœæ­¢")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='äººè„¸æƒ…æ„Ÿè¯†åˆ«å®¢æˆ·ç«¯')
    parser.add_argument('--server', default='http://frp-hub.com:45170', 
                       help='æœåŠ¡å™¨åœ°å€ (é»˜è®¤: http://frp-hub.com:45170)')
    parser.add_argument('--camera', type=int, default=0, 
                       help='æ‘„åƒå¤´ID (é»˜è®¤: 0)')
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¹¶å¯åŠ¨å®¢æˆ·ç«¯
    client = FaceEmotionClient(server_url=args.server)
    client.start()

if __name__ == '__main__':
    main()
