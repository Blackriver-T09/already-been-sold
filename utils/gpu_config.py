"""
GPUç¯å¢ƒé…ç½®å’Œä¼˜åŒ–æ¨¡å—
é’ˆå¯¹RTX 4070æ˜¾å¡çš„æœ€ä¼˜é…ç½®
"""

import os
import tensorflow as tf
import torch
import cv2
import numpy as np

def setup_gpu_environment():
    """
    è®¾ç½®GPUç¯å¢ƒå’Œå†…å­˜ç®¡ç†
    """
    print("ğŸš€ é…ç½®GPUç¯å¢ƒ...")
    
    # è®¾ç½®TensorFlow GPUå†…å­˜å¢é•¿
    try:
        gpus = tf.config.experimental.list_physical_devices('GPU')
        if gpus:
            # é€‰æ‹©å†…å­˜ç®¡ç†ç­–ç•¥ï¼šå†…å­˜å¢é•¿ OR è™šæ‹Ÿè®¾å¤‡é™åˆ¶ï¼ˆä¸èƒ½åŒæ—¶ä½¿ç”¨ï¼‰
            try:
                # æ–¹æ¡ˆ1ï¼šå¯ç”¨å†…å­˜å¢é•¿ï¼ˆæ¨èç”¨äºå®æ—¶å¤„ç†ï¼‰
                for gpu in gpus:
                    tf.config.experimental.set_memory_growth(gpu, True)
                print(f"âœ… TensorFlow GPUå†…å­˜å¢é•¿é…ç½®å®Œæˆ: {len(gpus)} GPU(s) å¯ç”¨")
            except Exception as memory_growth_error:
                print(f"âš ï¸ å†…å­˜å¢é•¿é…ç½®å¤±è´¥ï¼Œå°è¯•è™šæ‹Ÿè®¾å¤‡é…ç½®: {memory_growth_error}")
                try:
                    # æ–¹æ¡ˆ2ï¼šè®¾ç½®è™šæ‹ŸGPUè®¾å¤‡é™åˆ¶ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰
                    tf.config.experimental.set_virtual_device_configuration(
                        gpus[0],
                        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=8192)]  # 8GBé™åˆ¶
                    )
                    print(f"âœ… TensorFlow GPUè™šæ‹Ÿè®¾å¤‡é…ç½®å®Œæˆ: {len(gpus)} GPU(s) å¯ç”¨")
                except Exception as virtual_device_error:
                    print(f"âš ï¸ è™šæ‹Ÿè®¾å¤‡é…ç½®ä¹Ÿå¤±è´¥: {virtual_device_error}")
                    print("ğŸ”„ ä½¿ç”¨é»˜è®¤GPUé…ç½®")
        else:
            print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPU")
            
    except RuntimeError as e:
        print(f"âš ï¸ TensorFlow GPUé…ç½®å¤±è´¥: {e}")
    
    # è®¾ç½®PyTorch GPUé…ç½®
    try:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()  # æ¸…ç†GPUç¼“å­˜
            # è®¾ç½®CUDAå†…å­˜åˆ†é…ç­–ç•¥
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'
            print(f"âœ… PyTorch CUDAé…ç½®å®Œæˆ: {torch.cuda.device_count()} GPU(s) å¯ç”¨")
        else:
            print("âš ï¸ PyTorch CUDAä¸å¯ç”¨")
            
    except Exception as e:
        print(f"âš ï¸ PyTorch GPUé…ç½®å¤±è´¥: {e}")
    
    # è®¾ç½®OpenCV CUDAï¼ˆå¦‚æœå¯ç”¨ï¼‰
    try:
        if cv2.cuda.getCudaEnabledDeviceCount() > 0:
            print(f"âœ… OpenCV CUDAå¯ç”¨: {cv2.cuda.getCudaEnabledDeviceCount()} è®¾å¤‡")
        else:
            print("âš ï¸ OpenCV CUDAä¸å¯ç”¨")
    except:
        print("âš ï¸ OpenCV CUDAæ£€æŸ¥å¤±è´¥")

def get_optimal_deepface_config():
    """
    è·å–é’ˆå¯¹RTX 4070ä¼˜åŒ–çš„DeepFaceé…ç½®
    æ³¨æ„ï¼šç§»é™¤äº†ä¸å…¼å®¹çš„å‚æ•°ï¼ˆmodel_name, align, normalizationï¼‰
    """
    # ä½¿ç”¨æœ€ç¨³å®šçš„OpenCVæ£€æµ‹å™¨ï¼Œé¿å…æ‰€æœ‰åœ¨çº¿ä¸‹è½½å’Œå…¼å®¹æ€§é—®é¢˜
    config = {
        'detector_backend': 'opencv',  # æœ€ç¨³å®šçš„æ£€æµ‹å™¨ï¼Œå®Œå…¨æœ¬åœ°åŒ–
        'enforce_detection': False,    # å…è®¸ä½è´¨é‡å›¾åƒ
        # æ³¨æ„ï¼šä»¥ä¸‹å‚æ•°åœ¨å½“å‰ç‰ˆæœ¬ä¸­ä¸è¢«æ”¯æŒï¼Œå·²ç§»é™¤
        # 'model_name': 'Emotion',       # ä¸æ”¯æŒ
        # 'align': True,                 # ä¸æ”¯æŒ
        # 'normalization': 'base',       # ä¸æ”¯æŒ
    }
    
    # OpenCVæ£€æµ‹å™¨åœ¨GPUå’ŒCPUä¸Šéƒ½èƒ½ç¨³å®šå·¥ä½œ
    if tf.config.experimental.list_physical_devices('GPU'):
        print("ğŸš€ ä½¿ç”¨GPUåŠ é€Ÿæƒ…æ„Ÿè¯†åˆ« + OpenCVç¨³å®šæ£€æµ‹")
    else:
        print("ğŸ”„ ä½¿ç”¨CPUæƒ…æ„Ÿè¯†åˆ« + OpenCVç¨³å®šæ£€æµ‹")
    
    return config

def monitor_gpu_usage():
    """
    ç›‘æ§GPUä½¿ç”¨æƒ…å†µ
    """
    stats = {}
    
    try:
        # TensorFlow GPUä¿¡æ¯
        gpus = tf.config.experimental.list_physical_devices('GPU')
        stats['tf_gpus'] = len(gpus)
        
        # PyTorch GPUä¿¡æ¯
        if torch.cuda.is_available():
            stats['torch_cuda'] = True
            stats['torch_device_count'] = torch.cuda.device_count()
            stats['torch_current_device'] = torch.cuda.current_device()
            
            # GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
            stats['gpu_memory_allocated'] = torch.cuda.memory_allocated() / 1024**2  # MB
            stats['gpu_memory_reserved'] = torch.cuda.memory_reserved() / 1024**2    # MB
            stats['gpu_memory_total'] = torch.cuda.get_device_properties(0).total_memory / 1024**2  # MB
        else:
            stats['torch_cuda'] = False
            
        # OpenCV CUDAä¿¡æ¯
        stats['opencv_cuda_devices'] = cv2.cuda.getCudaEnabledDeviceCount()
        
    except Exception as e:
        stats['error'] = str(e)
    
    return stats

def optimize_gpu_for_realtime():
    """
    ä¸ºå®æ—¶å¤„ç†ä¼˜åŒ–GPUè®¾ç½®
    """
    try:
        # è®¾ç½®CUDAä¼˜åŒ–æ ‡å¿—
        os.environ['CUDA_LAUNCH_BLOCKING'] = '0'  # å¼‚æ­¥æ‰§è¡Œ
        os.environ['CUDA_CACHE_DISABLE'] = '0'    # å¯ç”¨ç¼“å­˜
        
        # TensorFlowä¼˜åŒ–
        tf.config.optimizer.set_jit(True)  # å¯ç”¨XLA JITç¼–è¯‘
        
        # è®¾ç½®çº¿ç¨‹å¹¶è¡Œ
        tf.config.threading.set_inter_op_parallelism_threads(4)
        tf.config.threading.set_intra_op_parallelism_threads(8)
        
        print("âœ… å®æ—¶å¤„ç†GPUä¼˜åŒ–å®Œæˆ")
        
    except Exception as e:
        print(f"âš ï¸ GPUä¼˜åŒ–è®¾ç½®å¤±è´¥: {e}")

def create_gpu_memory_pool():
    """
    åˆ›å»ºGPUå†…å­˜æ± ä»¥æé«˜æ€§èƒ½
    """
    try:
        if torch.cuda.is_available():
            # é¢„åˆ†é…ä¸€äº›GPUå†…å­˜
            dummy_tensor = torch.zeros(1024, 1024, device='cuda')
            del dummy_tensor
            torch.cuda.empty_cache()
            
        print("âœ… GPUå†…å­˜æ± åˆ›å»ºå®Œæˆ")
        
    except Exception as e:
        print(f"âš ï¸ GPUå†…å­˜æ± åˆ›å»ºå¤±è´¥: {e}")

def get_gpu_device_info():
    """
    è·å–è¯¦ç»†çš„GPUè®¾å¤‡ä¿¡æ¯
    """
    info = {}
    
    try:
        if torch.cuda.is_available():
            device_props = torch.cuda.get_device_properties(0)
            info['name'] = device_props.name
            info['total_memory'] = device_props.total_memory / 1024**2  # MB
            info['major'] = device_props.major
            info['minor'] = device_props.minor
            info['multi_processor_count'] = device_props.multi_processor_count
            
    except Exception as e:
        info['error'] = str(e)
    
    return info

# è‡ªåŠ¨åˆå§‹åŒ–GPUç¯å¢ƒ
if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•GPUé…ç½®...")
    setup_gpu_environment()
    optimize_gpu_for_realtime()
    create_gpu_memory_pool()
    
    print("\nğŸ“Š GPUè®¾å¤‡ä¿¡æ¯:")
    device_info = get_gpu_device_info()
    for key, value in device_info.items():
        print(f"  {key}: {value}")
    
    print("\nğŸ“ˆ GPUä½¿ç”¨ç»Ÿè®¡:")
    usage_stats = monitor_gpu_usage()
    for key, value in usage_stats.items():
        print(f"  {key}: {value}")
    
    print("\nğŸ”§ DeepFaceé…ç½®:")
    deepface_config = get_optimal_deepface_config()
    for key, value in deepface_config.items():
        print(f"  {key}: {value}")
