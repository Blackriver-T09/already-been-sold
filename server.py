import os
import warnings

# æŠ‘åˆ¶TensorFlowè­¦å‘Š
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
warnings.filterwarnings('ignore')

import cv2
import numpy as np
import base64
import time
import threading
from collections import deque, Counter
from flask import Flask, request
from flask_socketio import SocketIO, emit, disconnect
import json

# å¯¼å…¥ç°æœ‰çš„AIæ¨¡å—
from utils import *
from config import *
from emotion_tracker import *

# ğŸš€ å¯¼å…¥GPUåŠ é€Ÿçš„æƒ…æ„Ÿè¯†åˆ«æ¨¡å—
from utils.face_emotion_gpu import analyze_emotion_gpu, get_gpu_performance_stats
from utils.gpu_config import setup_gpu_environment, monitor_gpu_usage

# ğŸ†• å¯¼å…¥APIå‡½æ•°
from utils.API_picture import generate_poisonous_comment

# ğŸ§  å¯¼å…¥å†…å­˜ç®¡ç†å’Œæ–‡ä»¶æ¸…ç†æ¨¡å—
from utils.memory_manager import MemoryManager
from utils.file_cleaner import FileCleanupManager
from utils.emotion_tuning import emotion_tuning, apply_emotion_tuning, get_tuned_sensitivity_config

# åˆå§‹åŒ–GPUç¯å¢ƒ
setup_gpu_environment()
from utils.API_voice import generate_voice

# Flaskåº”ç”¨åˆå§‹åŒ–
app = Flask(__name__)
app.config['SECRET_KEY'] = 'your-secret-key-here'

# ğŸ†• å¢å¼ºSocketIOé…ç½®ä»¥æ”¯æŒHTTPéš§é“å’ŒUbuntuå…¼å®¹æ€§
socketio = SocketIO(
    app, 
    cors_allowed_origins="*", 
    async_mode='threading',
    # HTTPéš§é“ä¼˜åŒ–é…ç½®
    ping_timeout=60,        # å¢åŠ pingè¶…æ—¶æ—¶é—´
    ping_interval=25,       # å‡å°‘pingé—´éš”
    max_http_buffer_size=10**8,  # å¢åŠ ç¼“å†²åŒºå¤§å°æ”¯æŒå¤§è§†é¢‘å¸§
    allow_upgrades=True,    # å…è®¸åè®®å‡çº§
    # ğŸ§ Ubuntuå…¼å®¹æ€§é…ç½®
    engineio_logger=False,  # ç¦ç”¨engineioæ—¥å¿—é¿å…å†…éƒ¨é”™è¯¯
    logger=False,           # ç¦ç”¨socketioæ—¥å¿—
    always_connect=False    # é¿å…è¿æ¥çŠ¶æ€å†²çª
    # æ³¨æ„: transportså‚æ•°åœ¨è¾ƒæ—§ç‰ˆæœ¬ä¸­å¯èƒ½ä¸æ”¯æŒï¼Œå·²ç§»é™¤
)

# å…¨å±€å˜é‡åˆå§‹åŒ–ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
emotion_cache = {}
emotion_lock = threading.Lock()
emotion_history = {}
history_lock = threading.Lock()

# æœåŠ¡å™¨çŠ¶æ€ç®¡ç†
connected_clients = {}
processing_stats = {
    'total_frames': 0,
    'processed_frames': 0,
    'avg_processing_time': 0,
    'gpu_enabled': True,  # GPUåŠ é€ŸçŠ¶æ€
    'gpu_stats': {}       # GPUæ€§èƒ½ç»Ÿè®¡
}

class AIProcessor:
    """AIå¤„ç†å™¨ - å°è£…æ‰€æœ‰AIç›¸å…³åŠŸèƒ½"""
    
    def __init__(self):
        """åˆå§‹åŒ–AIå¤„ç†å™¨"""
        print("ğŸ¤– åˆå§‹åŒ–AIå¤„ç†å™¨...")
        
        # åˆå§‹åŒ–ç»„ä»¶ï¼ˆç§»æ¤è‡ªmain.pyï¼‰
        self.face_db = FaceDatabase(similarity_threshold=0.85, position_threshold=100)
        self.emotion_scheduler = EmotionScheduler(update_interval=0.3)
        self.system_controller = SystemController(
            self.emotion_scheduler, self.face_db, emotion_cache, 
            emotion_history, emotion_lock, history_lock
        )
        
        # åˆå§‹åŒ–å¿«ä¹ç¬é—´æ•æ‰ç®¡ç†å™¨
        self.happy_capture = HappyCaptureManager(
            capture_interval=15,  # ğŸ†• ä»20æ”¹ä¸º15ç§’ï¼Œä¸main.pyä¿æŒä¸€è‡´
            save_directory="pictures"
        )
        self.happy_capture.image_composer = ImageComposer(sources_dir="sources")
        
        # ğŸ†• è®¾ç½®å›è°ƒå‡½æ•°
        self.happy_capture.set_photo_callback(self.on_photo_taken)
        self.happy_capture.image_composer.set_composition_callback(self.on_photo_composed)
        
        # åˆå§‹åŒ–MediaPipe Face Meshï¼ˆæé«˜çµæ•åº¦ï¼‰
        self.face_mesh = mp_face_mesh.FaceMesh(
            max_num_faces=5,
            refine_landmarks=True,
            min_detection_confidence=0.3,  # ğŸ†• ä»0.5é™ä½åˆ°0.3ï¼Œæé«˜æ£€æµ‹çµæ•åº¦
            min_tracking_confidence=0.3   # ğŸ†• ä»0.5é™ä½åˆ°0.3ï¼Œæé«˜è·Ÿè¸ªçµæ•åº¦
        )
        
        # ğŸ†• å­˜å‚¨å½“å‰å¤„ç†çš„å®¢æˆ·ç«¯ID
        self.current_client_id = None
        
        # ğŸ†• æ—¶é—´æˆ³ç®¡ç† - è§£å†³MediaPipeæ—¶é—´æˆ³é”™è¯¯
        self.frame_timestamp = 0
        self.timestamp_lock = threading.Lock()
        
        # ğŸ§  åˆå§‹åŒ–å†…å­˜ç®¡ç†å™¨
        self.memory_manager = MemoryManager(
            emotion_cache=emotion_cache,
            emotion_history=emotion_history,
            emotion_lock=emotion_lock,
            history_lock=history_lock,
            max_face_cache_size=50,
            cache_expire_time=300,  # 5åˆ†é’Ÿ
            cleanup_interval=60     # 1åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
        )
        
        # ğŸ—‚ï¸ åˆå§‹åŒ–æ–‡ä»¶æ¸…ç†ç®¡ç†å™¨
        self.file_cleanup_manager = FileCleanupManager(
            pictures_dir="pictures",
            voice_dir="output_voice",
            max_files_per_dir=100,
            max_file_age_hours=24,
            cleanup_interval_minutes=30
        )
        
        # ğŸ¯ åŠ è½½æƒ…æ„Ÿç²¾è°ƒé…ç½®
        self.tuning_config = get_tuned_sensitivity_config()
        print(f"ğŸ¯ æƒ…æ„Ÿç²¾è°ƒé…ç½®å·²åŠ è½½: {self.tuning_config}")
        
        print("âœ… AIå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def set_current_client(self, client_id):
        """è®¾ç½®å½“å‰å¤„ç†çš„å®¢æˆ·ç«¯ID"""
        self.current_client_id = client_id
    
    def on_photo_taken(self, photo_info):
        """
        æ‹ç…§å®Œæˆå›è°ƒå‡½æ•°
        
        å‚æ•°:
            photo_info: dict - æ‹ç…§ä¿¡æ¯
        """
        try:
            print(f"ğŸ“¸ æ‹ç…§å®Œæˆï¼Œå¼€å§‹æ–°æµç¨‹: è¯„ä»· â†’ éŸ³é¢‘ â†’ å›¾ç‰‡åˆæˆ")
            
            # ğŸ†• è·å–åŸå§‹ç…§ç‰‡è·¯å¾„
            photo_path = photo_info.get('filepath', '')
            emotion_type = photo_info.get('emotion_type', 'unknown')
            
            if photo_path and os.path.exists(photo_path):
                # ğŸ†• é¡ºåºå¤„ç†ï¼šè¯„ä»· â†’ éŸ³é¢‘å’Œå›¾ç‰‡åˆæˆå¹¶è¡Œ
                def process_sequential_workflow():
                    """é¡ºåºå¤„ç†æµç¨‹ï¼šå…ˆè¯„ä»·ï¼Œå†å¹¶è¡ŒéŸ³é¢‘å’Œå›¾ç‰‡åˆæˆ"""
                    try:
                        print(f"ğŸ¤– æ­¥éª¤1: å¼€å§‹ç”Ÿæˆå›¾ç‰‡è¯„ä»·: {photo_path}")
                        
                        # æ­¥éª¤1: ç”Ÿæˆæ¯’èˆŒè¯„ä»·
                        comment = generate_poisonous_comment(photo_path)
                        
                        if comment:
                            print(f"ğŸ’¬ æ­¥éª¤1å®Œæˆ - è¯„ä»·ç”ŸæˆæˆåŠŸ: {comment}")
                            
                            # ç”ŸæˆéŸ³é¢‘æ–‡ä»¶åï¼ˆåŸºäºç…§ç‰‡æ–‡ä»¶åï¼Œä¸åŠ å‰ç¼€ï¼Œgenerate_voiceå‡½æ•°ä¼šè‡ªåŠ¨æ·»åŠ voice_å‰ç¼€ï¼‰
                            photo_filename = photo_info.get('filename', '')
                            if photo_filename.endswith('.jpg'):
                                audio_filename = photo_filename.replace('.jpg', '.wav')
                            else:
                                audio_filename = f"{photo_filename}.wav"
                            
                            # ç”Ÿæˆæœ€ç»ˆçš„éŸ³é¢‘æ–‡ä»¶åï¼ˆç”¨äºå‘é€æ—¶æŸ¥æ‰¾ï¼‰
                            final_audio_filename = f"voice_{audio_filename}"
                            
                            # ğŸ†• æ­¥éª¤2: éŸ³é¢‘ç”Ÿæˆå’Œå›¾ç‰‡åˆæˆçš„åŒæ­¥æµç¨‹
                            import threading
                            from threading import Event
                            
                            # åˆ›å»ºåŒæ­¥äº‹ä»¶
                            audio_ready_event = Event()
                            composition_ready_event = Event()
                            
                            def generate_audio():
                                try:
                                    print(f"ğŸ”Š æ­¥éª¤2a: å¼€å§‹ç”ŸæˆéŸ³é¢‘: {audio_filename}")
                                    generate_voice(comment, audio_filename)
                                    print(f"âœ… æ­¥éª¤2aå®Œæˆ - éŸ³é¢‘ç”ŸæˆæˆåŠŸ")
                                    audio_ready_event.set()  # æ ‡è®°éŸ³é¢‘å·²å‡†å¤‡å°±ç»ª
                                except Exception as e:
                                    print(f"âŒ éŸ³é¢‘ç”Ÿæˆå¤±è´¥: {e}")
                                    audio_ready_event.set()  # å³ä½¿å¤±è´¥ä¹Ÿè¦è®¾ç½®äº‹ä»¶
                            
                            def generate_image_composition():
                                try:
                                    print(f"ğŸ¨ æ­¥éª¤2b: å¼€å§‹å›¾ç‰‡åˆæˆ: {photo_path}")
                                    # æ‰‹åŠ¨è§¦å‘å›¾ç‰‡åˆæˆï¼ˆå› ä¸ºæˆ‘ä»¬é˜»æ­¢äº†è‡ªåŠ¨åˆæˆï¼‰
                                    self.happy_capture.image_composer.queue_composition(photo_path, emotion_type)
                                    print(f"âœ… æ­¥éª¤2bå®Œæˆ - å›¾ç‰‡åˆæˆå·²è§¦å‘")
                                    composition_ready_event.set()  # æ ‡è®°å›¾ç‰‡åˆæˆå·²å‡†å¤‡å°±ç»ª
                                except Exception as e:
                                    print(f"âŒ å›¾ç‰‡åˆæˆå¤±è´¥: {e}")
                                    composition_ready_event.set()  # å³ä½¿å¤±è´¥ä¹Ÿè¦è®¾ç½®äº‹ä»¶
                            
                            def sync_audio_and_display():
                                """ç­‰å¾…éŸ³é¢‘ç”Ÿæˆå®Œæˆåï¼Œå‘é€éŸ³é¢‘å¹¶è§¦å‘å›¾ç‰‡å±•ç¤ºåŒæ­¥"""
                                try:
                                    # ç­‰å¾…éŸ³é¢‘ç”Ÿæˆå®Œæˆ
                                    audio_ready_event.wait(timeout=30)  # æœ€å¤šç­‰å¾…30ç§’
                                    
                                    # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆä½¿ç”¨æœ€ç»ˆçš„éŸ³é¢‘æ–‡ä»¶åï¼‰
                                    audio_path = os.path.join('output_voice', final_audio_filename)
                                    if os.path.exists(audio_path):
                                        print(f"ğŸµ æ­¥éª¤3: éŸ³é¢‘å°±ç»ªï¼Œå‘é€ç»™å®¢æˆ·ç«¯å¹¶å¼€å§‹éŸ³ç”»åŒæ­¥")
                                        
                                        # è¯»å–éŸ³é¢‘æ–‡ä»¶å¹¶è½¬æ¢ä¸ºbase64
                                        with open(audio_path, 'rb') as audio_file:
                                            audio_data = audio_file.read()
                                            audio_b64 = base64.b64encode(audio_data).decode('utf-8')
                                        
                                        # å‘é€éŸ³é¢‘å’ŒåŒæ­¥æŒ‡ä»¤ç»™å®¢æˆ·ç«¯
                                        if self.current_client_id:
                                            socketio.emit('audio_and_display_sync', {
                                                'comment': comment,
                                                'audio_filename': final_audio_filename,  # ä½¿ç”¨æœ€ç»ˆçš„éŸ³é¢‘æ–‡ä»¶å
                                                'audio_data': audio_b64,
                                                'photo_path': photo_path,
                                                'emotion_type': emotion_type,
                                                'start_display': True,  # æŒ‡ç¤ºå®¢æˆ·ç«¯å¼€å§‹å±•ç¤ºå›¾ç‰‡
                                                'timestamp': time.time()
                                            }, room=self.current_client_id)
                                            print(f"ğŸ“¤ éŸ³é¢‘å·²å‘é€ç»™å®¢æˆ·ç«¯ï¼Œå›¾ç‰‡å±•ç¤ºå·²åŒæ­¥å¯åŠ¨")
                                    else:
                                        print(f"âš ï¸ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡éŸ³é¢‘å‘é€: {audio_path}")
                                        
                                except Exception as e:
                                    print(f"âŒ éŸ³ç”»åŒæ­¥å¤„ç†å¤±è´¥: {e}")
                            
                            # å¹¶è¡Œå¯åŠ¨éŸ³é¢‘ç”Ÿæˆå’Œå›¾ç‰‡åˆæˆ
                            audio_thread = threading.Thread(target=generate_audio)
                            image_thread = threading.Thread(target=generate_image_composition)
                            sync_thread = threading.Thread(target=sync_audio_and_display)
                            
                            audio_thread.daemon = True
                            image_thread.daemon = True
                            sync_thread.daemon = True
                            
                            audio_thread.start()
                            image_thread.start()
                            sync_thread.start()
                            
                            # ğŸ†• æ—§çš„è¯„ä»·ä¿¡æ¯å‘é€å·²è¢«æ–°çš„éŸ³ç”»åŒæ­¥æœºåˆ¶æ›¿ä»£
                                
                        else:
                            print(f"âŒ æ­¥éª¤1å¤±è´¥ - è¯„ä»·ç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡åç»­æ­¥éª¤")
                            # å¦‚æœè¯„ä»·ç”Ÿæˆå¤±è´¥ï¼Œä»ç„¶è¿›è¡Œå›¾ç‰‡åˆæˆ
                            self.happy_capture.image_composer.queue_composition(photo_path, emotion_type)
                            
                    except Exception as e:
                        print(f"âŒ æ•´ä½“æµç¨‹å¤„ç†å¤±è´¥: {e}")
                        # å¦‚æœå‡ºé”™ï¼Œä»ç„¶è¿›è¡Œå›¾ç‰‡åˆæˆ
                        try:
                            self.happy_capture.image_composer.queue_composition(photo_path, emotion_type)
                        except:
                            pass
                
                # ğŸ†• åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­å¤„ç†æ•´ä¸ªæµç¨‹
                workflow_thread = threading.Thread(target=process_sequential_workflow)
                workflow_thread.daemon = True
                workflow_thread.start()
                
            # ğŸ†• å‘å®¢æˆ·ç«¯å‘é€æ‹ç…§é€šçŸ¥
            if self.current_client_id:
                socketio.emit('photo_taken', {
                    'message': 'ç…§ç‰‡æ‹æ‘„å®Œæˆï¼æ­£åœ¨ç”Ÿæˆè¯„ä»·...',
                    'emotion_type': photo_info.get('emotion_type', 'unknown'),
                    'emotion_score': photo_info.get('emotion_score', 0),
                    'filename': photo_info.get('filename', ''),
                    'timestamp': time.time()
                }, room=self.current_client_id)
                
        except Exception as e:
            print(f"âŒ æ‹ç…§å›è°ƒå¤„ç†å¤±è´¥: {e}")
    
    def on_photo_composed(self, composition_info):
        """
        å›¾ç‰‡åˆæˆå®Œæˆå›è°ƒå‡½æ•°
        
        å‚æ•°:
            composition_info: dict - åˆæˆä¿¡æ¯
        """
        try:
            print(f"ğŸ¨ å‘å®¢æˆ·ç«¯å‘é€åˆæˆå®Œæˆé€šçŸ¥: {self.current_client_id}")
            
            if self.current_client_id and composition_info.get('success'):
                # è¯»å–åˆæˆåçš„å›¾ç‰‡
                composed_image_path = composition_info.get('output_path')
                if composed_image_path and os.path.exists(composed_image_path):
                    # è¯»å–å¹¶ç¼–ç å›¾ç‰‡
                    composed_image = cv2.imread(composed_image_path)
                    if composed_image is not None:
                        # ç¼–ç ä¸ºbase64
                        _, buffer = cv2.imencode('.jpg', composed_image, [cv2.IMWRITE_JPEG_QUALITY, 90])
                        composed_image_b64 = base64.b64encode(buffer).decode('utf-8')
                        
                        socketio.emit('photo_composed', {
                            'message': 'ç…§ç‰‡åˆæˆå®Œæˆï¼',
                            'composed_image': composed_image_b64,
                            'display_duration': 5,  # æ˜¾ç¤º5ç§’
                            'emotion_type': composition_info.get('emotion_type', 'unknown'),
                            'overlay_type': composition_info.get('overlay_type', 'normal'),
                            'timestamp': time.time()
                        }, room=self.current_client_id)
                    else:
                        print(f"âŒ æ— æ³•è¯»å–åˆæˆå›¾ç‰‡: {composed_image_path}")
                else:
                    print(f"âŒ åˆæˆå›¾ç‰‡è·¯å¾„æ— æ•ˆ: {composed_image_path}")
                    
        except Exception as e:
            print(f"âŒ å‘é€åˆæˆå®Œæˆé€šçŸ¥å¤±è´¥: {e}")
    
    def process_frame(self, frame_data):
        """
        å¤„ç†è§†é¢‘å¸§
        
        å‚æ•°:
            frame_data: dict - åŒ…å«å›¾åƒæ•°æ®å’Œå…ƒä¿¡æ¯
        
        è¿”å›:
            dict: å¤„ç†ç»“æœ
        """
        start_time = time.time()
        
        try:
            # è§£ç å›¾åƒ
            image_bytes = base64.b64decode(frame_data['image'])
            nparr = np.frombuffer(image_bytes, np.uint8)
            original_image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
            
            if original_image is None:
                return self._error_response("å›¾åƒè§£ç å¤±è´¥")
            
            # åˆ›å»ºæ˜¾ç¤ºå›¾åƒå‰¯æœ¬
            display_image = original_image.copy()
            
            # ğŸ†• æ—¶é—´æˆ³ç®¡ç† - ç¡®ä¿MediaPipeæ—¶é—´æˆ³ä¸¥æ ¼é€’å¢
            with self.timestamp_lock:
                self.frame_timestamp += 1
                current_timestamp = self.frame_timestamp
            
            # äººè„¸æ£€æµ‹ - ä½¿ç”¨å¸¦é”™è¯¯å¤„ç†çš„ç‰ˆæœ¬
            try:
                results = BGR_RGB(display_image, self.face_mesh)
            except Exception as mp_error:
                print(f"âš ï¸ MediaPipeå¤„ç†é”™è¯¯: {str(mp_error)}")
                # é‡æ–°åˆå§‹åŒ–MediaPipeä»¥æ¢å¤
                self.face_mesh = mp_face_mesh.FaceMesh(
                    max_num_faces=5,
                    refine_landmarks=True,
                    min_detection_confidence=0.5,
                    min_tracking_confidence=0.5
                )
                # é‡è¯•ä¸€æ¬¡
                try:
                    results = BGR_RGB(display_image, self.face_mesh)
                except Exception as retry_error:
                    print(f"âŒ MediaPipeé‡è¯•å¤±è´¥: {str(retry_error)}")
                    return self._error_response(f"MediaPipeå¤„ç†å¤±è´¥: {str(retry_error)}")
            detected_faces = process_detection_results(results, display_image.shape)
            
            # æ”¶é›†å½“å‰å¸§çš„æ‰€æœ‰äººè„¸æ•°æ®
            current_faces_data = []
            faces_info = []
            
            # å¤„ç†æ¯å¼ æ£€æµ‹åˆ°çš„äººè„¸
            for face_info in detected_faces:
                # äººè„¸åŒ¹é…
                matched_id = process_face_matching(
                    face_info, self.face_db, self._reset_emotion_history
                )
                
                # ğŸ¨ ç»˜åˆ¶äººè„¸ç½‘æ ¼
                draw_face(display_image, face_info['landmarks'])
                
                # æå–äººè„¸åŒºåŸŸè¿›è¡Œæƒ…æ„Ÿåˆ†æ
                face_img = extract_face_region(original_image, face_info['bbox'])
                
                # ğŸš€ è°ƒåº¦GPUåŠ é€Ÿæƒ…æ„Ÿåˆ†æ
                self.emotion_scheduler.schedule_emotion_analysis(
                    matched_id, face_img, emotion_lock, emotion_cache, analyze_emotion_gpu
                )
                
                # è·å–æƒ…æ„Ÿæ•°æ®
                emotion_data = self._get_emotion_data(matched_id)
                if emotion_data:
                    # æ”¶é›†äººè„¸æ•°æ®
                    current_faces_data.append({
                        'face_id': matched_id,
                        'emotion_data': emotion_data,
                        'face_info': face_info
                    })
                    
                    # ğŸ¨ ç»˜åˆ¶æƒ…æ„Ÿä¿¡æ¯
                    x, y, w, h = face_info['bbox']
                    draw_emotion_bars(
                        display_image, emotion_data['all_emotions'], x, y, h,
                        matched_id, emotion_data['text'], 
                        emotion_data['color'], emotion_colors
                    )
                    
                    # ğŸ¨ ç»˜åˆ¶è¾¹ç•Œæ¡†
                    cv2.rectangle(display_image, (x, y), (x + w, y + h), emotion_data['color'], 2)
                    
                    # æ›´æ–°æƒ…æ„Ÿå˜åŒ–è®°å½•
                    self.emotion_scheduler.update_emotion_change(matched_id, emotion_data['emotion'])
                    
                    # æ·»åŠ åˆ°è¿”å›æ•°æ®
                    faces_info.append({
                        'face_id': matched_id,
                        'bbox': [x, y, w, h],
                        'emotion': emotion_data['emotion'],
                        'score': emotion_data['score'],
                        'all_emotions': emotion_data['all_emotions'],
                        'color': emotion_data['color']
                    })
            
            # ğŸ†• å¤„ç†å¿«ä¹ç¬é—´æ•æ‰
            self.happy_capture.capture_happy_moment(original_image, current_faces_data)
            
            # ğŸ†• ç»˜åˆ¶æ•æ‰å€’è®¡æ—¶ä¿¡æ¯
            self.happy_capture.draw_countdown_info(display_image)
            
            # ğŸ†• ç»˜åˆ¶æ•æ‰å¯è§†åŒ–æŒ‡ç¤º
            if self.happy_capture.last_capture_visual_indicator:
                self.happy_capture.draw_capture_visual_on_display(display_image, current_faces_data)
            
            # ç»˜åˆ¶ç³»ç»Ÿä¿¡æ¯
            self.system_controller.draw_system_info(display_image)
            
            # ç¼–ç å¤„ç†åçš„å›¾åƒ
            _, buffer = cv2.imencode('.jpg', display_image, [cv2.IMWRITE_JPEG_QUALITY, 80])
            processed_image_b64 = base64.b64encode(buffer).decode('utf-8')
            
            # æ›´æ–°å¤„ç†ç»Ÿè®¡
            processing_time = time.time() - start_time
            self._update_stats(processing_time)
            
            return {
                'success': True,
                'frame_id': frame_data.get('frame_id', 0),
                'timestamp': time.time(),
                'processed_image': processed_image_b64,
                'faces': faces_info,
                'processing_time': processing_time,
                'stats': {
                    'total_faces': len(faces_info),
                    'server_fps': 1.0 / processing_time if processing_time > 0 else 0
                }
            }
            
        except Exception as e:
            print(f"âŒ å¤„ç†è§†é¢‘å¸§æ—¶å‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()
            return self._error_response(f"å¤„ç†å¤±è´¥: {str(e)}")
    
    def _get_emotion_data(self, matched_id):
        """è·å–æƒ…æ„Ÿæ•°æ®"""
        # ğŸ§  è®°å½•äººè„¸è®¿é—®ï¼Œç”¨äºå†…å­˜ç®¡ç†
        self.memory_manager.record_face_access(matched_id)
        
        with emotion_lock:
            if matched_id in emotion_cache:
                emotion_data = emotion_cache[matched_id]
                
                emotion = emotion_data['dominant_emotion']
                score = emotion_data['dominant_score']
                all_emotions = emotion_data.get('all_emotions', {})
                
                # ğŸ¯ åº”ç”¨æƒ…æ„Ÿç²¾è°ƒé…ç½®
                tuned_emotions = apply_emotion_tuning(all_emotions)
                
                # é‡æ–°è®¡ç®—ä¸»å¯¼æƒ…æ„Ÿ
                if tuned_emotions:
                    dominant_emotion = max(tuned_emotions, key=tuned_emotions.get)
                    dominant_score = tuned_emotions[dominant_emotion]
                    
                    emotion = dominant_emotion
                    score = dominant_score
                    all_emotions = tuned_emotions
                
                emotion_text = f"{emotion.capitalize()}({score:.1f})"
                emotion_color = emotion_colors.get(emotion, (255, 255, 255))
                
                return {
                    'emotion': emotion,
                    'score': score,
                    'text': emotion_text,
                    'color': emotion_color,
                    'all_emotions': all_emotions
                }
        return None
    
    def _reset_emotion_history(self, face_id):
        """é‡ç½®æƒ…æ„Ÿå†å²"""
        with history_lock:
            if face_id in emotion_history:
                emotion_history[face_id].clear()
    
    def _error_response(self, message):
        """è¿”å›é”™è¯¯å“åº”"""
        return {
            'success': False,
            'error': message,
            'timestamp': time.time()
        }
    
    def _update_stats(self, processing_time):
        """æ›´æ–°å¤„ç†ç»Ÿè®¡"""
        processing_stats['total_frames'] += 1
        processing_stats['processed_frames'] += 1
        
        # è®¡ç®—ç§»åŠ¨å¹³å‡å¤„ç†æ—¶é—´
        alpha = 0.1  # å¹³æ»‘å› å­
        if processing_stats['avg_processing_time'] == 0:
            processing_stats['avg_processing_time'] = processing_time
        else:
            processing_stats['avg_processing_time'] = (
                alpha * processing_time + 
                (1 - alpha) * processing_stats['avg_processing_time']
            )

# å…¨å±€AIå¤„ç†å™¨å®ä¾‹
ai_processor = AIProcessor()

# WebSocketäº‹ä»¶å¤„ç†
@socketio.on('connect')
def handle_connect():
    """å®¢æˆ·ç«¯è¿æ¥äº‹ä»¶"""
    client_id = request.sid
    connected_clients[client_id] = {
        'connect_time': time.time(),
        'frames_received': 0,
        'last_frame_time': 0
    }
    print(f"ğŸ”— å®¢æˆ·ç«¯è¿æ¥: {client_id}")
    emit('connection_response', {
        'status': 'connected',
        'client_id': client_id,
        'server_time': time.time()
    })

@socketio.on('disconnect')
def handle_disconnect():
    """å®¢æˆ·ç«¯æ–­å¼€äº‹ä»¶"""
    client_id = request.sid
    if client_id in connected_clients:
        del connected_clients[client_id]
    print(f"âŒ å®¢æˆ·ç«¯æ–­å¼€: {client_id}")

@socketio.on('video_frame')
def handle_video_frame(data):
    """å¤„ç†è§†é¢‘å¸§"""
    client_id = request.sid
    
    try:
        # ğŸ†• è®¾ç½®å½“å‰å®¢æˆ·ç«¯ID
        ai_processor.set_current_client(client_id)
        
        # æ›´æ–°å®¢æˆ·ç«¯ç»Ÿè®¡
        if client_id in connected_clients:
            connected_clients[client_id]['frames_received'] += 1
            connected_clients[client_id]['last_frame_time'] = time.time()
        
        # ğŸ†• æ·»åŠ æ•°æ®éªŒè¯
        if not data or 'image' not in data:
            print(f"âš ï¸ å®¢æˆ·ç«¯ {client_id} å‘é€äº†æ— æ•ˆçš„è§†é¢‘å¸§æ•°æ®")
            # ğŸ§ Ubuntuå…¼å®¹æ€§: ä½¿ç”¨try-exceptåŒ…è£¹emit
            try:
                emit('error', {'message': 'æ— æ•ˆçš„è§†é¢‘å¸§æ•°æ®'})
            except Exception as emit_error:
                print(f"âš ï¸ emité”™è¯¯å¤±è´¥: {emit_error}")
            return
        
        # å¤„ç†è§†é¢‘å¸§
        result = ai_processor.process_frame(data)
        
        # ğŸ†• æ£€æŸ¥å¤„ç†ç»“æœ
        if result and result.get('success', False):
            # ğŸ§ Ubuntuå…¼å®¹æ€§: ä½¿ç”¨try-exceptåŒ…è£¹emit
            try:
                emit('processed_frame', result)
            except Exception as emit_error:
                print(f"âš ï¸ emitå¤„ç†ç»“æœå¤±è´¥: {emit_error}")
        else:
            error_msg = result.get('error', 'æœªçŸ¥å¤„ç†é”™è¯¯') if result else 'å¤„ç†ç»“æœä¸ºç©º'
            print(f"âš ï¸ è§†é¢‘å¸§å¤„ç†å¤±è´¥: {error_msg}")
            # ğŸ§ Ubuntuå…¼å®¹æ€§: ä½¿ç”¨try-exceptåŒ…è£¹emit
            try:
                emit('error', {'message': error_msg})
            except Exception as emit_error:
                print(f"âš ï¸ emité”™è¯¯å¤±è´¥: {emit_error}")
        
    except Exception as e:
        error_msg = f"å¤„ç†è§†é¢‘å¸§æ—¶å‡ºé”™: {str(e)}"
        print(f"âŒ {error_msg}")
        
        # ğŸ§ Ubuntuå…¼å®¹æ€§: ä½¿ç”¨try-exceptåŒ…è£¹emit
        try:
            emit('error', {'message': error_msg})
        except Exception as emit_error:
            print(f"âš ï¸ emité”™è¯¯å¤±è´¥: {emit_error}")
        
        # ğŸ†• å¦‚æœæ˜¯MediaPipeç›¸å…³é”™è¯¯ï¼Œå°è¯•é‡ç½®AIå¤„ç†å™¨
        if 'MediaPipe' in str(e) or 'timestamp' in str(e).lower():
            print("ğŸ”„ æ£€æµ‹åˆ°MediaPipeé”™è¯¯ï¼Œå°è¯•é‡ç½®AIå¤„ç†å™¨...")
            try:
                ai_processor.face_mesh = mp_face_mesh.FaceMesh(
                    max_num_faces=5,
                    refine_landmarks=True,
                    min_detection_confidence=0.5,
                    min_tracking_confidence=0.5
                )
                print("âœ… AIå¤„ç†å™¨é‡ç½®æˆåŠŸ")
            except Exception as reset_error:
                print(f"âŒ AIå¤„ç†å™¨é‡ç½®å¤±è´¥: {str(reset_error)}")

@socketio.on('ping')
def handle_ping(data):
    """å¤„ç†ping"""
    emit('pong', {
        'client_timestamp': data.get('timestamp', 0),
        'server_timestamp': time.time()
    })

@socketio.on('get_stats')
def handle_get_stats():
    """è·å–æœåŠ¡å™¨ç»Ÿè®¡ä¿¡æ¯"""
    # ğŸš€ æ›´æ–°GPUæ€§èƒ½ç»Ÿè®¡
    try:
        processing_stats['gpu_stats'] = get_gpu_performance_stats()
        gpu_usage = monitor_gpu_usage()
        processing_stats['gpu_usage'] = gpu_usage
    except Exception as e:
        processing_stats['gpu_error'] = str(e)
    
    emit('stats_response', {
        'processing_stats': processing_stats,
        'connected_clients': len(connected_clients),
        'server_time': time.time()
    })

@socketio.on('get_gpu_stats')
def handle_get_gpu_stats():
    """è·å–GPUæ€§èƒ½ç»Ÿè®¡"""
    try:
        gpu_stats = get_gpu_performance_stats()
        gpu_usage = monitor_gpu_usage()
        emit('gpu_stats_response', {
            'gpu_performance': gpu_stats,
            'gpu_usage': gpu_usage,
            'timestamp': time.time()
        })
    except Exception as e:
        emit('gpu_stats_response', {
            'error': str(e),
            'timestamp': time.time()
        })

@socketio.on('get_memory_stats')
def handle_get_memory_stats():
    """è·å–å†…å­˜ç®¡ç†ç»Ÿè®¡"""
    try:
        memory_stats = ai_processor.memory_manager.get_memory_stats()
        emit('memory_stats_response', {
            'memory_stats': memory_stats,
            'timestamp': time.time()
        })
    except Exception as e:
        emit('memory_stats_response', {
            'error': str(e),
            'timestamp': time.time()
        })

@socketio.on('get_file_stats')
def handle_get_file_stats():
    """è·å–æ–‡ä»¶æ¸…ç†ç»Ÿè®¡"""
    try:
        file_stats = ai_processor.file_cleanup_manager.get_directory_stats()
        emit('file_stats_response', {
            'file_stats': file_stats,
            'timestamp': time.time()
        })
    except Exception as e:
        emit('file_stats_response', {
            'error': str(e),
            'timestamp': time.time()
        })

@socketio.on('force_cleanup')
def handle_force_cleanup(data):
    """å¼ºåˆ¶æ¸…ç†"""
    try:
        cleanup_type = data.get('type', 'all')
        
        if cleanup_type in ['memory', 'all']:
            ai_processor.memory_manager.force_cleanup()
        
        if cleanup_type in ['files', 'all']:
            ai_processor.file_cleanup_manager.force_cleanup()
        
        emit('cleanup_response', {
            'success': True,
            'type': cleanup_type,
            'message': f'{cleanup_type}æ¸…ç†å®Œæˆ',
            'timestamp': time.time()
        })
    except Exception as e:
        emit('cleanup_response', {
            'success': False,
            'error': str(e),
            'timestamp': time.time()
        })

@socketio.on('get_emotion_tuning')
def handle_get_emotion_tuning():
    """è·å–æƒ…æ„Ÿç²¾è°ƒé…ç½®"""
    try:
        config_summary = emotion_tuning.get_config_summary()
        emit('emotion_tuning_response', {
            'config': config_summary,
            'timestamp': time.time()
        })
    except Exception as e:
        emit('emotion_tuning_response', {
            'error': str(e),
            'timestamp': time.time()
        })

@socketio.on('update_emotion_tuning')
def handle_update_emotion_tuning(data):
    """æ›´æ–°æƒ…æ„Ÿç²¾è°ƒé…ç½®"""
    try:
        update_type = data.get('type')
        values = data.get('values', {})
        
        if update_type == 'weights':
            emotion_tuning.update_emotion_weights(values)
        elif update_type == 'biases':
            emotion_tuning.update_emotion_biases(values)
        elif update_type == 'probability_adjustments':
            emotion_tuning.update_probability_adjustments(values)
        elif update_type == 'preset':
            preset_name = data.get('preset_name')
            emotion_tuning.create_preset_config(preset_name)
        else:
            raise ValueError(f'æœªçŸ¥çš„æ›´æ–°ç±»å‹: {update_type}')
        
        # æ›´æ–°AIå¤„ç†å™¨çš„é…ç½®
        ai_processor.tuning_config = get_tuned_sensitivity_config()
        
        emit('tuning_update_response', {
            'success': True,
            'type': update_type,
            'message': 'æƒ…æ„Ÿç²¾è°ƒé…ç½®å·²æ›´æ–°',
            'timestamp': time.time()
        })
    except Exception as e:
        emit('tuning_update_response', {
            'success': False,
            'error': str(e),
            'timestamp': time.time()
        })

# HTTPè·¯ç”±
@app.route('/')
def index():
    """æœåŠ¡å™¨çŠ¶æ€é¡µé¢"""
    return f"""
    <h1>ğŸ¤– äººè„¸æƒ…æ„Ÿè¯†åˆ«æœåŠ¡å™¨</h1>
    <p><strong>çŠ¶æ€:</strong> è¿è¡Œä¸­</p>
    <p><strong>è¿æ¥å®¢æˆ·ç«¯:</strong> {len(connected_clients)}</p>
    <p><strong>å¤„ç†å¸§æ•°:</strong> {processing_stats['processed_frames']}</p>
    <p><strong>å¹³å‡å¤„ç†æ—¶é—´:</strong> {processing_stats['avg_processing_time']:.3f}s</p>
    <p><strong>æœåŠ¡å™¨æ—¶é—´:</strong> {time.strftime('%Y-%m-%d %H:%M:%S')}</p>
    """

@app.route('/health')
def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        'status': 'healthy',
        'timestamp': time.time(),
        'connected_clients': len(connected_clients),
        'processing_stats': processing_stats
    }

if __name__ == '__main__':
    print("ğŸš€ å¯åŠ¨äººè„¸æƒ…æ„Ÿè¯†åˆ«æœåŠ¡å™¨...")
    
    # ğŸš€ æ˜¾ç¤ºGPUçŠ¶æ€
    try:
        gpu_stats = get_gpu_performance_stats()
        gpu_usage = monitor_gpu_usage()
        print(f"ğŸ“Š GPUçŠ¶æ€: {gpu_stats.get('gpu_available', False)}")
        print(f"ğŸ“Š CUDAå¯ç”¨: {gpu_stats.get('cuda_available', False)}")
        if gpu_usage.get('torch_cuda'):
            print(f"ğŸ“Š GPUè®¾å¤‡: {gpu_usage.get('torch_device_count', 0)} ä¸ª")
            print(f"ğŸ“Š GPUå†…å­˜: {gpu_usage.get('gpu_memory_total', 0):.0f}MB")
        print(f"ğŸ¤– DeepFaceé…ç½®: {gpu_stats.get('deepface_config', {})}")
        processing_stats['gpu_stats'] = gpu_stats
        processing_stats['gpu_usage'] = gpu_usage
    except Exception as e:
        print(f"âš ï¸ GPUçŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}")
        processing_stats['gpu_enabled'] = False
    
    print("ğŸ“¡ WebSocketæœåŠ¡å™¨: http://localhost:7861")
    print("ğŸŒ çŠ¶æ€é¡µé¢: http://localhost:7861")
    print("â¤ï¸ å¥åº·æ£€æŸ¥: http://localhost:7861/health")
    print("ğŸš€ æœåŠ¡å™¨å¯åŠ¨ä¸­...")
    
    socketio.run(app, host='0.0.0.0', port=7861, debug=False, use_reloader=False)
